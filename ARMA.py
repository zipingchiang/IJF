# -*- coding: utf-8 -*-
"""ARMA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z7j3saKMUkGN9erXBdbPYmje_gpSGs4i
"""

from dataclasses import dataclass, field
from typing import Protocol, Tuple, Optional, Callable
import numpy as np
import matplotlib.pyplot as plt

@dataclass
class GeometryLoading:
    sigma0: float = 1.0     # reference stress
    ramp: float = 0.0       # linear ramp rate in time
    omega: float = 0.0      # sinusoidal frequency
    cyc_frac: float = 0.0   # sinusoid amplitude fraction
    Y0: float = 1.0         # baseline geometry factor
    a_crit: float = 2.0     # characteristic length that bounds Y(a)

def sigma_of_time(t: float, load: GeometryLoading) -> float:
    return load.sigma0 * (1.0 + load.ramp * t) * (1.0 + load.cyc_frac * np.sin(load.omega * t))

def Y_of_a(a: float, load: GeometryLoading) -> float:
    # Bounded geometry factor to avoid singular Y as a -> large
    return load.Y0 / np.sqrt(1.0 + (max(a, 0.0) / load.a_crit))

def K_of_ta(t: float, a: float, load: GeometryLoading) -> float:
    # Mode-I style K with simple geometry factor and sigma(t)
    return Y_of_a(a, load) * sigma_of_time(t, load) * np.sqrt(np.pi * max(a, 0.0))

class DrivingLaw(Protocol):
    def n_internal(self) -> int: ...
    def rhs(self, t: float, a: float, z: np.ndarray) -> Tuple[float, np.ndarray, float]:
        ...

@dataclass
class VE_Geff_Params:
    Eprime: float = 1.0
    w: np.ndarray = field(default_factory=lambda: np.array([0.6, 0.3, 0.1]))
    tau: np.ndarray = field(default_factory=lambda: np.array([0.1, 1.0, 5.0]))

class ViscoelasticGeff(DrivingLaw):
    def __init__(self, params: VE_Geff_Params, load: GeometryLoading):
        self.p = params
        self.load = load

    def n_internal(self) -> int:
        return len(self.p.w)

    def G_inst(self, t: float, a: float) -> float:
        K = K_of_ta(t, a, self.load)
        return (K**2) / self.p.Eprime

    def rhs(self, t: float, a: float, z: np.ndarray) -> Tuple[float, np.ndarray, float]:
        G = self.G_inst(t, a)
        dz = -z / self.p.tau + G
        G_eff = float((self.p.w * z).sum())
        return G_eff, dz, G

@dataclass
class Cstar_Params:
    C0: float = 1.0
    p_sigma: float = 2.0
    q_a: float = 0.0
    use_K: bool = False
    p_K: float = 2.0

class CstarDriver(DrivingLaw):
    def __init__(self, params: Cstar_Params, load: GeometryLoading):
        self.p = params
        self.load = load

    def n_internal(self) -> int:
        return 0

    def C_star(self, t: float, a: float) -> float:
        if self.p.use_K:
            K = K_of_ta(t, a, self.load)
            return self.p.C0 * (max(K, 0.0) ** self.p.p_K)
        sig = sigma_of_time(t, self.load)
        return self.p.C0 * (max(sig, 0.0) ** self.p.p_sigma) * (max(a, 0.0) ** self.p.q_a)

    def rhs(self, t: float, a: float, z: np.ndarray) -> Tuple[float, np.ndarray, float]:
        Cstar = self.C_star(t, a)
        dz = np.zeros(0)
        return Cstar, dz, Cstar

@dataclass
class J_Params:
    Eprime: float = 1.0      # plane strain elastic modulus E/(1-nu^2)
    Kprime: float = 1.0      # Ramberg–Osgood strength parameter
    n: float = 5.0           # hardening exponent (>1). RO: eps = sig/E + (sig/K')^{1/n}
    CJ: float = 1.0          # geometry/constraint factor for plastic J
    include_J_elastic: bool = True  # add Jel = K^2/E'

class JDriver(DrivingLaw):
    def __init__(self, params: J_Params, load: GeometryLoading):
        self.p = params
        self.load = load

    def n_internal(self) -> int:
        return 0

    def J_of_ta(self, t: float, a: float) -> float:
        a_pos = max(a, 0.0)
        sig = max(sigma_of_time(t, self.load), 0.0)
        Jel = 0.0
        if self.p.include_J_elastic:
            K = max(K_of_ta(t, a_pos, self.load), 0.0)
            Jel = (K**2) / self.p.Eprime
        Jpl = self.p.CJ * a_pos * (sig ** (self.p.n + 1)) / (self.p.Kprime ** self.p.n)
        return Jel + Jpl

    def rhs(self, t: float, a: float, z: np.ndarray) -> Tuple[float, np.ndarray, float]:
        J = self.J_of_ta(t, a)
        dz = np.zeros(0)
        return J, dz, J

# =========================
# Sublinear crack-growth law
# =========================

@dataclass
class CrackGrowthLaw:
    A_rate: float = 1.0
    m_sub: float = 0.5   # 0<m<1 introduces non-Lipschitz near threshold
    threshold: float = 0.2

def crack_rate(drive_effective: float, law: CrackGrowthLaw) -> float:
    drive = max(drive_effective - law.threshold, 0.0)
    if drive <= 0.0:
        return 0.0
    return law.A_rate * (drive ** law.m_sub)


class NoiseModel:
    def reset(self): ...
    def step(self, h: float) -> float: ...

class WhiteNoise(NoiseModel):
    def __init__(self, sigma: float, seed: Optional[int] = None):
        self.sigma = float(sigma)
        self.rng = np.random.default_rng(seed)

    def reset(self):
        pass

    def step(self, h: float) -> float:
        return self.sigma * np.sqrt(max(h, 0.0)) * self.rng.standard_normal()

class OrnsteinUhlenbeck(NoiseModel):
    def __init__(self, sigma: float, tau: float, seed: Optional[int] = None):
        self.sigma = float(sigma)
        self.tau = float(tau)
        self.rng = np.random.default_rng(seed)
        self.x = 0.0

    def reset(self):
        self.x = 0.0

    def step(self, h: float) -> float:
        if h <= 0.0:
            return 0.0
        drift = -self.x * (h / self.tau)
        diff = np.sqrt(2.0 * self.sigma**2 * h / self.tau) * self.rng.standard_normal()
        self.x += drift + diff
        return self.x

class StochasticDriver(DrivingLaw):
    def __init__(self, base: DrivingLaw, noise: Optional[NoiseModel]):
        self.base = base
        self.noise = noise
        self._last_xi = 0.0

    def n_internal(self) -> int:
        return self.base.n_internal()

    def new_step(self, h: float):
        self._last_xi = 0.0 if self.noise is None else self.noise.step(h)

    def rhs(self, t: float, a: float, z: np.ndarray) -> Tuple[float, np.ndarray, float]:
        drive, dz, _ = self.base.rhs(t, a, z)
        drive_noisy = drive + self._last_xi
        return drive_noisy, dz, drive_noisy

def rk4_step_withlog(fun: Callable, t: float, y: np.ndarray, h: float):
    k1, d1 = fun(t, y)
    k2, _  = fun(t + 0.5*h, y + 0.5*h*k1)
    k3, _  = fun(t + 0.5*h, y + 0.5*h*k2)
    k4, _  = fun(t + h,     y + h*k3)
    y_next = y + (h/6.0)*(k1 + 2*k2 + 2*k3 + k4)
    return y_next, d1

def integrate_rk4_log(t0: float, tf: float, y0: np.ndarray, h: float,
                      driver: StochasticDriver, law: CrackGrowthLaw):
    n = int(np.ceil((tf - t0) / h))
    T = np.empty(n + 1)
    Y = np.empty((n + 1, len(y0)))
    D = np.empty(n + 1)
    t = t0
    y = y0.copy()

    for i in range(n + 1):
        T[i] = t
        Y[i] = y
        # log current driver at grid (uses last noise from previous step)
        drive_now, _, _ = driver.rhs(t, float(y[0]), y[1:])
        D[i] = drive_now

        if i < n:
            # sample noise for the next RK step
            driver.new_step(h)

            def f(ts, ys):
                a = float(max(ys[0], 0.0))
                z = ys[1:]
                drive_eff, dz, _ = driver.rhs(ts, a, z)
                da = crack_rate(drive_eff, law)
                if dz.size > 0:
                    return np.concatenate(([da], dz)), drive_eff
                return np.array([da]), drive_eff

            y, _ = rk4_step_withlog(f, t, y, h)
            t += h

    return T, Y, D

@dataclass
class CrackGrowthLawHetero:
    A_rate_mu: float = 1.0
    A_rate_cv: float = 0.0
    threshold_mu: float = 0.2
    threshold_cv: float = 0.0
    m_sub: float = 0.5

    def sample(self, rng: np.random.Generator) -> CrackGrowthLaw:
        # Lognormal sampling if CV>0, else deterministic mean.
        if self.A_rate_cv > 0:
            A = rng.lognormal(mean=np.log(self.A_rate_mu) - 0.5*np.log(1 + self.A_rate_cv**2),
                              sigma=np.sqrt(np.log(1 + self.A_rate_cv**2)))
        else:
            A = self.A_rate_mu
        if self.threshold_cv > 0:
            Dc = rng.lognormal(mean=np.log(self.threshold_mu) - 0.5*np.log(1 + self.threshold_cv**2),
                               sigma=np.sqrt(np.log(1 + self.threshold_cv**2)))
        else:
            Dc = self.threshold_mu
        return CrackGrowthLaw(A_rate=A, m_sub=self.m_sub, threshold=Dc)

def first_passage_time(T: np.ndarray, Y: np.ndarray, D: np.ndarray,
                       law: CrackGrowthLaw, eps: float = 1e-6) -> float:
    above = (D > law.threshold) & (Y[:, 0] > eps)
    if np.any(above):
        idx = int(np.argmax(above))
        return float(T[idx])
    return np.inf


def run_ensemble(driver_factory: Callable[[], DrivingLaw],
                 law_hetero: CrackGrowthLawHetero,
                 y0: np.ndarray, t0: float, tf: float, h: float,
                 noise: Optional[NoiseModel],
                 N: int = 200, seed: int = 1111,
                 eps: float = 1e-6) -> np.ndarray:
    rng = np.random.default_rng(seed)
    Ti = np.empty(N)
    for i in range(N):
        law = law_hetero.sample(rng)
        base = driver_factory()
        # per-sample randomized inputs (open intervals) if ranges provided
        if sigma0_range is not None and Y0_range is not None:
            eps_open = 1e-12
            lo_s, hi_s = float(sigma0_range[0]), float(sigma0_range[1])
            lo_y, hi_y = float(Y0_range[0]), float(Y0_range[1])
            u1 = rng.random(); u2 = rng.random()
            sigma0_i = (lo_s + eps_open) + (hi_s - lo_s - 2*eps_open) * u1
            Y0_i    = (lo_y + eps_open) + (hi_y - lo_y - 2*eps_open) * u2
            if hasattr(base, 'load') and base.load is not None:
                try:
                    base.load = type(base.load)(**{**base.load.__dict__, 'sigma0': sigma0_i, 'Y0': Y0_i})
                except Exception:
                    base.load.sigma0 = sigma0_i
                    base.load.Y0 = Y0_i
        sdriver = StochasticDriver(base, noise)
        if noise is not None:
            noise.reset()
        T, Y, D = integrate_rk4_log(t0, tf, y0, h, sdriver, law)
        Ti[i] = first_passage_time(T, Y, D, law, eps=eps)
    return Ti

def plot_ecdf(samples: np.ndarray, outpath: str,
              xlabel: str = r"Incubation time $T_\mathrm{i}$"):
    x = np.sort(samples[np.isfinite(samples)])
    if x.size == 0:
        x = np.array([0.0])
        y = np.array([0.0])
    else:
        y = np.arange(1, x.size + 1) / x.size
    plt.figure(figsize=(7.0, 4.5))
    plt.step(x, y, where='post')
    plt.xlabel(xlabel)
    plt.ylabel('Empirical CDF')
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(outpath, dpi=160)
    plt.close()

def plot_hist(samples: np.ndarray, outpath: str, bins: int = 30,
              xlabel: str = r"Incubation time $T_\mathrm{i}$"):
    x = samples[np.isfinite(samples)]
    if x.size == 0:
        # Avoid numpy histogram warnings when empty
        x = np.array([0.0])
    plt.figure(figsize=(7.0, 4.5))
    plt.hist(x, bins=bins, density=True)
    plt.xlabel(xlabel)
    plt.ylabel('Density')
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(outpath, dpi=160)
    plt.close()

def demo_viscoelastic_stochastic(N: int = 300, tf: float = 12.0, h: float = 0.002,
                                 out_ecdf: str = "stoch_VE_ecdf.png",
                                 out_hist: str = "stoch_VE_hist.png",
                                 eps: float = 1e-6) -> np.ndarray:
    load = GeometryLoading(sigma0=0.9, ramp=0.0, Y0=1.0, a_crit=2.0)

    def factory():
        return ViscoelasticGeff(VE_Geff_Params(Eprime=1.0), load)

    law_het = CrackGrowthLawHetero(A_rate_mu=1.0, A_rate_cv=0.15,
                                   threshold_mu=0.2, threshold_cv=0.10, m_sub=0.5)
    noise = OrnsteinUhlenbeck(sigma=0.02, tau=0.5, seed=1111)

    base = factory()
    y0 = np.concatenate(([1e-8], np.zeros(base.n_internal())))
    Ti = run_ensemble(factory, law_het, y0, t0=0.0, tf=tf, h=h,
                      noise=noise, N=N, seed=1111, eps=eps)
    plot_ecdf(Ti, out_ecdf)
    plot_hist(Ti, out_hist)
    return Ti

def demo_cstar_stochastic(N: int = 300, tf: float = 12.0, h: float = 0.002,
                          out_ecdf: str = "stoch_Cstar_ecdf.png",
                          out_hist: str = "stoch_Cstar_hist.png",
                          eps: float = 1e-6) -> np.ndarray:
    load = GeometryLoading(sigma0=0.9, ramp=0.0, Y0=1.0, a_crit=2.0)

    def factory():
        return CstarDriver(Cstar_Params(C0=0.5, p_sigma=3.0, q_a=0.2, use_K=False), load)

    law_het = CrackGrowthLawHetero(A_rate_mu=1.6, A_rate_cv=0.10,
                                   threshold_mu=0.1, threshold_cv=0.08, m_sub=0.5)
    noise = WhiteNoise(sigma=0.2, seed=1111)

    y0 = np.array([1e-6])  # No internal memory states in C* branch
    Ti = run_ensemble(factory, law_het, y0, t0=0.0, tf=tf, h=h,
                      noise=noise, N=N, seed=1111, eps=eps)
    plot_ecdf(Ti, out_ecdf)
    plot_hist(Ti, out_hist)
    return Ti


def demo_J_plastic_stochastic(N: int = 300, tf: float = 12.0, h: float = 0.002,
                              out_ecdf: str = "stoch_J_ecdf.png",
                              out_hist: str = "stoch_J_hist.png",
                              eps: float = 1e-8) -> np.ndarray:
    load = GeometryLoading(sigma0=0.9, ramp=0.0, Y0=1.0, a_crit=2.0)

    def factory():
        return JDriver(J_Params(Eprime=1.0, Kprime=2.0, n=5.0, CJ=0.5, include_J_elastic=True), load)

    law_het = CrackGrowthLawHetero(A_rate_mu=1.5, A_rate_cv=0.10,
                                   threshold_mu=0.12, threshold_cv=0.05, m_sub=0.5)
    noise = WhiteNoise(sigma=0.2, seed=1111)

    y0 = np.array([1e-6])
    Ti = run_ensemble(factory, law_het, y0, t0=0.0, tf=tf, h=h,
                      noise=noise, N=N, seed=1111, eps=eps)
    plot_ecdf(Ti, out_ecdf)
    plot_hist(Ti, out_hist)
    return Ti

import os
import csv
from typing import NamedTuple

from typing import NamedTuple

class FirstPassageResult(NamedTuple):
    t_fp: float  # first-passage time (np.inf if never crosses)
    a_fp: float  # crack size at first passage (np.nan if never crosses)
    idx: int     # index into the logged arrays (or -1 if never crosses)


def first_passage_info(T: np.ndarray, Y: np.ndarray, D: np.ndarray,
                       law: CrackGrowthLaw, eps: float = 1e-6) -> FirstPassageResult:
    above = (D > law.threshold) & (Y[:, 0] > eps)
    if np.any(above):
        idx = int(np.argmax(above))
        return FirstPassageResult(float(T[idx]), float(Y[idx, 0]), idx)
    return FirstPassageResult(np.inf, float('nan'), -1)


def run_ensemble_with_size(driver_factory: Callable[[], DrivingLaw],
                           law_hetero: CrackGrowthLawHetero,
                           y0: np.ndarray, t0: float, tf: float, h: float,
                           noise: Optional[NoiseModel],
                           N: int = 200, seed: int = 1111,
                           eps: float = 1e-6) -> Tuple[np.ndarray, np.ndarray]:
    rng = np.random.default_rng(seed)
    Ti = np.empty(N)
    Ai = np.empty(N)
    for i in range(N):
        law = law_hetero.sample(rng)
        base = driver_factory()
        # per-sample randomized inputs (open intervals) if ranges provided
        if sigma0_range is not None and Y0_range is not None:
            eps_open = 1e-12
            lo_s, hi_s = float(sigma0_range[0]), float(sigma0_range[1])
            lo_y, hi_y = float(Y0_range[0]), float(Y0_range[1])
            u1 = rng.random(); u2 = rng.random()
            sigma0_i = (lo_s + eps_open) + (hi_s - lo_s - 2*eps_open) * u1
            Y0_i    = (lo_y + eps_open) + (hi_y - lo_y - 2*eps_open) * u2
            if hasattr(base, 'load') and base.load is not None:
                try:
                    base.load = type(base.load)(**{**base.load.__dict__, 'sigma0': sigma0_i, 'Y0': Y0_i})
                except Exception:
                    base.load.sigma0 = sigma0_i
                    base.load.Y0 = Y0_i
        sdriver = StochasticDriver(base, noise)
        if noise is not None:
            noise.reset()
        T, Y, D = integrate_rk4_log(t0, tf, y0, h, sdriver, law)
        res = first_passage_info(T, Y, D, law, eps=eps)
        Ti[i] = res.t_fp
        Ai[i] = res.a_fp
    return Ti, Ai

# Convenience plotting of crack size at first passage

def plot_a_hist(Ai: np.ndarray, outpath: str, bins: int = 30,
                xlabel: str = r"Crack size at first passage $a_*$"):
    x = Ai[np.isfinite(Ai)]
    x = x[~np.isnan(x)]
    if x.size == 0:
        x = np.array([0.0])
    plt.figure(figsize=(7.0, 4.5))
    plt.hist(x, bins=bins, density=True)
    plt.xlabel(xlabel)
    plt.ylabel('Density')
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(outpath, dpi=160)
    plt.close()

def save_summary_csv(path: str, Ti: np.ndarray, Ai: np.ndarray) -> None:
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    with open(path, "w", newline="") as f:
        w = csv.writer(f)
        w.writerow(["sample_id",
            "sigma0","ramp","omega","cyc_frac",
            "Y0","a_crit",
            "A_rate","D_c","m_sub",
            "noise_sigma","seed","a0","h","tf",
            "t_fp","a_fp","finite"])  # header
        for i, (t, a) in enumerate(zip(Ti, Ai)):
            finite = int(np.isfinite(t))
            # Keep NaN for a if not finite, so downstream can filter
            w.writerow([i, repr(float(t)), repr(float(a)) if np.isfinite(a) else "nan", finite])

def run_ensemble_with_size_csv(driver_factory: Callable[[], DrivingLaw],
                               law_hetero: CrackGrowthLawHetero,
                               y0: np.ndarray, t0: float, tf: float, h: float,
                               noise: Optional[NoiseModel],
                               N: int = 200, seed: int = 1111,
                               eps: float = 1e-6,
                               out_csv: Optional[str] = None,
                               traj_dir: Optional[str] = None,
                               traj_limit: int = 0,
                               sigma0_range: tuple | None = None,
                               Y0_range: tuple | None = None) -> Tuple[np.ndarray, np.ndarray]:
    rng = np.random.default_rng(seed)
    Ti = np.empty(N)
    Ai = np.empty(N)

    if traj_dir is not None:
        os.makedirs(traj_dir, exist_ok=True)

    rows = [] if out_csv else None

    for i in range(N):
        law = law_hetero.sample(rng)
        base = driver_factory()
        # per-sample randomized inputs (open intervals) if ranges provided
        if sigma0_range is not None and Y0_range is not None:
            eps_open = 1e-12
            lo_s, hi_s = float(sigma0_range[0]), float(sigma0_range[1])
            lo_y, hi_y = float(Y0_range[0]), float(Y0_range[1])
            u1 = rng.random(); u2 = rng.random()
            sigma0_i = (lo_s + eps_open) + (hi_s - lo_s - 2*eps_open) * u1
            Y0_i    = (lo_y + eps_open) + (hi_y - lo_y - 2*eps_open) * u2
            if hasattr(base, 'load') and base.load is not None:
                try:
                    base.load = type(base.load)(**{**base.load.__dict__, 'sigma0': sigma0_i, 'Y0': Y0_i})
                except Exception:
                    base.load.sigma0 = sigma0_i
                    base.load.Y0 = Y0_i
        sdriver = StochasticDriver(base, noise)
        if noise is not None:
            noise.reset()
        T, Y, D = integrate_rk4_log(t0, tf, y0, h, sdriver, law)
        res = first_passage_info(T, Y, D, law, eps=eps)
        Ti[i] = res.t_fp
        Ai[i] = res.a_fp

        if traj_dir is not None and i < traj_limit:
            # Save trajectory of crack size only, with time
            a_series = Y[:, 0]
            with open(os.path.join(traj_dir, f"traj_{i}.csv"), "w", newline="") as f:
                w = csv.writer(f)
                w.writerow(["t", "a"])  # header
                for tt, aa in zip(T, a_series):
                    w.writerow([repr(float(tt)), repr(float(aa))])

        if rows is not None:
            finite = int(np.isfinite(res.t_fp))
            _load = base.load if hasattr(base, 'load') else None
            _sigma0 = getattr(_load, 'sigma0', float('nan'))
            _ramp = getattr(_load, 'ramp', float('nan'))
            _omega = getattr(_load, 'omega', float('nan'))
            _cyc = getattr(_load, 'cyc_frac', float('nan'))
            _Y0 = getattr(_load, 'Y0', float('nan'))
            _acrit = getattr(_load, 'a_crit', float('nan'))
            _A = getattr(law, 'A_rate', float('nan'))
            _Dc = getattr(law, 'threshold', float('nan'))
            _m = getattr(law, 'm_sub', float('nan'))
            _ns = getattr(noise, 'sigma', float('nan')) if noise is not None else float('nan')
            rows.append([
                i,
                repr(float(_sigma0)), repr(float(_ramp)), repr(float(_omega)), repr(float(_cyc)),
                repr(float(_Y0)), repr(float(_acrit)),
                repr(float(_A)), repr(float(_Dc)), repr(float(_m)),
                repr(float(_ns)), repr(int(seed)), repr(float(y0[0])), repr(float(h)), repr(float(tf)),
                repr(float(res.t_fp)), repr(float(res.a_fp)) if np.isfinite(res.a_fp) else 'nan', finite
            ])
    if out_csv:
        if rows is None:
            save_summary_csv(out_csv, Ti, Ai)
        else:
            os.makedirs(os.path.dirname(out_csv) or ".", exist_ok=True)
            with open(out_csv, "w", newline="") as _f:
                w = csv.writer(_f)
                w.writerow(["sample_id",
            "sigma0","ramp","omega","cyc_frac",
            "Y0","a_crit",
            "A_rate","D_c","m_sub",
            "noise_sigma","seed","a0","h","tf",
            "t_fp","a_fp","finite"])
                w.writerows(rows)
    return Ti, Ai

def demo_J_plastic_stochastic_with_size_csv(N: int = 300, tf: float = 12.0, h: float = 0.002,
                                             out_ecdf: str = "stoch_J_ecdf.png",
                                             out_hist: str = "stoch_J_hist.png",
                                             out_ahist: str = "stoch_J_a_hist.png",
                                             out_csv: Optional[str] = None,
                                             traj_dir: Optional[str] = None,
                                             traj_limit: int = 0,
                                             eps: float = 1e-8) -> Tuple[np.ndarray, np.ndarray]:
    load = GeometryLoading(sigma0=0.9, ramp=0.0, Y0=1.0, a_crit=2.0)
    def factory():
        return JDriver(J_Params(Eprime=1.0, Kprime=2.0, n=5.0, CJ=0.5, include_J_elastic=True), load)
    law_het = CrackGrowthLawHetero(A_rate_mu=1.5, A_rate_cv=0.10,
                                   threshold_mu=0.12, threshold_cv=0.05, m_sub=0.5)
    noise = WhiteNoise(sigma=0.2, seed=1111)
    y0 = np.array([1e-6])
    Ti, Ai = run_ensemble_with_size_csv(factory, law_het, y0, t0=0.0, tf=tf, h=h,
                                        noise=noise, N=N, seed=1111, eps=eps,
                                        out_csv=out_csv, traj_dir=traj_dir, traj_limit=traj_limit, sigma0_range=(0.5, 0.9), Y0_range=(0.8, 1.2))
    plot_ecdf(Ti, out_ecdf)
    plot_hist(Ti, out_hist)
    plot_a_hist(Ai, out_ahist)
    return Ti, Ai

def demo_viscoelastic_stochastic_with_size_csv(N: int = 300, tf: float = 12.0, h: float = 0.002,
                                                out_ecdf: str = "stoch_VE_ecdf.png",
                                                out_hist: str = "stoch_VE_hist.png",
                                                out_ahist: str = "stoch_VE_a_hist.png",
                                                out_csv: Optional[str] = None,
                                                traj_dir: Optional[str] = None,
                                                traj_limit: int = 0,
                                                eps: float = 1e-6) -> Tuple[np.ndarray, np.ndarray]:
    load = GeometryLoading(sigma0=0.9, ramp=0.0, Y0=1.0, a_crit=2.0)
    def factory():
        return ViscoelasticGeff(VE_Geff_Params(Eprime=1.0), load)
    law_het = CrackGrowthLawHetero(A_rate_mu=1.0, A_rate_cv=0.15,
                                   threshold_mu=0.2, threshold_cv=0.10, m_sub=0.5)
    noise = OrnsteinUhlenbeck(sigma=0.02, tau=0.5, seed=1111)
    base = factory()
    y0 = np.concatenate(([1e-8], np.zeros(base.n_internal())))
    Ti, Ai = run_ensemble_with_size_csv(factory, law_het, y0, t0=0.0, tf=tf, h=h,
                                        noise=noise, N=N, seed=1111, eps=eps,
                                        out_csv=out_csv, traj_dir=traj_dir, traj_limit=traj_limit, sigma0_range=(0.5, 0.9), Y0_range=(0.8, 1.2))
    plot_ecdf(Ti, out_ecdf)
    plot_hist(Ti, out_hist)
    plot_a_hist(Ai, out_ahist)
    return Ti, Ai

def demo_cstar_stochastic_with_size_csv(N: int = 300, tf: float = 12.0, h: float = 0.002,
                                         out_ecdf: str = "stoch_Cstar_ecdf.png",
                                         out_hist: str = "stoch_Cstar_hist.png",
                                         out_ahist: str = "stoch_Cstar_a_hist.png",
                                         out_csv: Optional[str] = None,
                                         traj_dir: Optional[str] = None,
                                         traj_limit: int = 0,
                                         eps: float = 1e-6) -> Tuple[np.ndarray, np.ndarray]:
    load = GeometryLoading(sigma0=0.9, ramp=0.0, Y0=1.0, a_crit=2.0)
    def factory():
        return CstarDriver(Cstar_Params(C0=0.5, p_sigma=3.0, q_a=0.2, use_K=False), load)
    law_het = CrackGrowthLawHetero(A_rate_mu=1.6, A_rate_cv=0.10,
                                   threshold_mu=0.1, threshold_cv=0.08, m_sub=0.5)
    noise = WhiteNoise(sigma=0.2, seed=1111)
    y0 = np.array([1e-6])
    Ti, Ai = run_ensemble_with_size_csv(factory, law_het, y0, t0=0.0, tf=tf, h=h,
                                        noise=noise, N=N, seed=1111, eps=eps,
                                        out_csv=out_csv, traj_dir=traj_dir, traj_limit=traj_limit, sigma0_range=(0.5, 0.9), Y0_range=(0.8, 1.2))
    plot_ecdf(Ti, out_ecdf)
    plot_hist(Ti, out_hist)
    plot_a_hist(Ai, out_ahist)
    return Ti, Ai


Ti, Ai = demo_J_plastic_stochastic_with_size_csv(
    N=6000,
    tf=6.5,
    h=0.2,
    out_ecdf="stoch_J_ecdf.png",
    out_hist="stoch_J_hist.png",
    out_ahist="stoch_J_a_hist.png",
    out_csv="nonunique_6000_io.csv",
    traj_dir=None,
    traj_limit=0,
    eps=1e-8
)

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


CSV_PATH = "nonunique_6000_io.csv"
OUTDIR = "fig"

X_COLS = [
    "sigma0", "ramp", "omega", "cyc_frac",
    "Y0", "A_rate", "D_c", "m_sub", "a0", "h"
]

TARGETS = ["t_fp", "a_fp"]

def safe_mkdir(path):
    os.makedirs(path, exist_ok=True)


def load_and_clean(csv_path):
    df = pd.read_csv(csv_path)

    required = set(X_COLS + TARGETS)
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f"Missing columns in CSV: {missing}")

    return df[X_COLS + TARGETS].dropna().reset_index(drop=True)


def report_duplicate_X(df):
    g = df.groupby(X_COLS, dropna=False)
    counts = g.size().rename("n_rows").reset_index()

    dup = counts[counts["n_rows"] > 1]

    print("====== Duplicate-X report ======")
    print("Total rows:", len(df))
    print("Unique X:", len(counts))
    print("X with multiple Y:", len(dup))
    if len(dup) > 0:
        print("\nTop duplicated X:")
        print(dup.sort_values("n_rows", ascending=False).head(10))
    print("================================\n")


# =======================
# Figure A
# =======================

def fig_group_index_scatter(df, outdir, max_groups=300):
    g = df.groupby(X_COLS, sort=False)
    counts = g.size().sort_values(ascending=False)

    groups = counts.index[:max_groups]
    group_to_idx = {g: i for i, g in enumerate(groups)}

    sub = df[df[X_COLS].apply(tuple, axis=1).isin(group_to_idx)].copy()

    x_base = sub[X_COLS].apply(tuple, axis=1).map(group_to_idx).to_numpy()
    jitter = np.random.default_rng(0).uniform(-0.2, 0.2, size=len(x_base))
    x = x_base + jitter

    for y in TARGETS:
        plt.figure(figsize=(8, 4))
        plt.scatter(x, sub[y], s=10)
        plt.xlabel("Unique X group index")
        plt.ylabel(y)
        plt.tight_layout()
        plt.savefig(os.path.join(outdir, f"A_group_index_{y}.png"), dpi=200)
        plt.close()





def main():
    safe_mkdir(OUTDIR)

    df = load_and_clean(CSV_PATH)

    report_duplicate_X(df)

    fig_group_index_scatter(df, OUTDIR)

    print("All figures saved to:", OUTDIR)


if __name__ == "__main__":
    main()

!pip -q install pandas numpy scikit-learn torch tqdm

import os, math, random, time
import numpy as np
import pandas as pd
from tqdm.auto import tqdm

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(42)
device = "cuda" if torch.cuda.is_available() else "cpu"
print("device =", device)

CSV_PATH = "nonunique_6000_io.csv"

X_COLS = ["sigma0", "ramp", "omega", "cyc_frac", "Y0", "A_rate", "D_c", "m_sub", "a0", "h"]
TARGETS = [("t_fp", 0), ("a_fp", 1)]

EPOCHS_SINGLE_DEEP = 200      # Transformer, ToyMambaSSM (single-valued)
EPOCHS_DIST_DEEP   = 200      # MLP-MDN, Transformer-MDN, ToyMambaSSM-MDN
BATCH_SIZE         = 256


# ---------------------------
# Load data
# ---------------------------
if not os.path.exists(CSV_PATH):
    raise FileNotFoundError(f"Cannot find {CSV_PATH}. Please upload it to Colab working directory.")

df = pd.read_csv(CSV_PATH)
missing = [c for c in X_COLS + [t for t, _ in TARGETS] if c not in df.columns]
if missing:
    raise ValueError(f"CSV missing columns: {missing}")

df = df[X_COLS + [t for t, _ in TARGETS]].copy()
df = df.dropna(subset=[t for t, _ in TARGETS])

X = df[X_COLS].to_numpy(dtype=np.float32)
Y = df[[t for t, _ in TARGETS]].to_numpy(dtype=np.float32)  # shape (N,2)

X_train_raw, X_test_raw, Y_train_2, Y_test_2 = train_test_split(
    X, Y, test_size=0.2, random_state=42
)

print("Train:", X_train_raw.shape, Y_train_2.shape)
print("Test :", X_test_raw.shape,  Y_test_2.shape)


# ---------------------------
# Metrics helper (single output)
# ---------------------------
def single_target_metrics(ytr, yte, yhat_tr, yhat_te, model_name, target_name, seconds=None, extra=None):
    rec = {
        "model": model_name,
        "target": target_name,
        "train_R2": float(r2_score(ytr, yhat_tr)),
        "test_R2":  float(r2_score(yte, yhat_te)),
        "train_MSE": float(mean_squared_error(ytr, yhat_tr)),
        "test_MSE":  float(mean_squared_error(yte, yhat_te)),
    }
    if seconds is not None:
        rec["seconds"] = float(seconds)
    if extra:
        for k, v in extra.items():
            rec[k] = float(v)
    return rec

imp = SimpleImputer(strategy="median")
sc = StandardScaler()

X_train_scaled = sc.fit_transform(imp.fit_transform(X_train_raw))
X_test_scaled  = sc.transform(imp.transform(X_test_raw))

Xtr_t = torch.tensor(X_train_scaled, dtype=torch.float32)
Xte_t = torch.tensor(X_test_scaled, dtype=torch.float32)

class ParamTransformerReg(nn.Module):
    def __init__(self, n_tokens=10, d_model=64, nhead=4, n_layers=2, out_dim=1):
        super().__init__()
        self.embed = nn.Linear(1, d_model)
        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=4*d_model,
            batch_first=True, activation="gelu", dropout=0.1
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)
        self.head = nn.Sequential(
            nn.Linear(n_tokens * d_model, 128),
            nn.GELU(),
            nn.Linear(128, out_dim)
        )

    def forward(self, x):
        z = self.embed(x.unsqueeze(-1))
        z = self.encoder(z)
        z = z.flatten(1)
        return self.head(z)


class ToyMambaSSM(nn.Module):
    def __init__(self, d_model=64):
        super().__init__()
        self.Wx = nn.Linear(d_model, d_model)
        self.Wh = nn.Linear(d_model, d_model, bias=False)
        self.Wg = nn.Linear(d_model, d_model)

    def forward(self, x):
        B, T, D = x.shape
        h = torch.zeros(B, D, device=x.device, dtype=x.dtype)
        outs = []
        for t in range(T):
            xt = x[:, t, :]
            g = torch.sigmoid(self.Wg(xt))
            h_tilde = torch.tanh(self.Wx(xt) + self.Wh(h))
            h = (1.0 - g) * h + g * h_tilde
            outs.append(h.unsqueeze(1))
        return torch.cat(outs, dim=1)


class ToyMambaReg(nn.Module):
    def __init__(self, n_tokens=10, d_model=64, n_layers=2, out_dim=1):
        super().__init__()
        self.embed = nn.Linear(1, d_model)
        self.layers = nn.ModuleList([ToyMambaSSM(d_model=d_model) for _ in range(n_layers)])
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Sequential(
            nn.Linear(n_tokens * d_model, 128),
            nn.GELU(),
            nn.Linear(128, out_dim)
        )

    def forward(self, x):
        z = self.embed(x.unsqueeze(-1))
        for layer in self.layers:
            z = z + layer(z)
        z = self.norm(z)
        z = z.flatten(1)
        return self.head(z)


def train_torch_regressor_single_target(model, y_train_1d, epochs=200, lr=1e-3, wd=1e-4):
    """
    model: outputs shape (B,1)
    y_train_1d: numpy shape (N,)
    """
    Ytr1 = torch.tensor(y_train_1d.reshape(-1, 1), dtype=torch.float32)
    train_loader = DataLoader(TensorDataset(Xtr_t, Ytr1), batch_size=BATCH_SIZE, shuffle=True)

    model = model.to(device)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)
    loss_fn = nn.MSELoss()

    pbar = tqdm(range(epochs), desc=f"[Single] {model.__class__.__name__}", leave=True)
    for _ in pbar:
        model.train()
        running = 0.0
        n = 0
        for xb, yb in train_loader:
            xb = xb.to(device)
            yb = yb.to(device)
            opt.zero_grad()
            pred = model(xb)
            loss = loss_fn(pred, yb)
            loss.backward()
            opt.step()
            running += float(loss.item())
            n += 1
        pbar.set_postfix(loss=f"{(running/max(n,1)):.4g}")

    model.eval()
    with torch.no_grad():
        pred_tr = model(Xtr_t.to(device)).cpu().numpy().reshape(-1)
        pred_te = model(Xte_t.to(device)).cpu().numpy().reshape(-1)
    return pred_tr, pred_te


print("\nRunning Part A: single-valued regression (split targets)...")

single_records = []

jobs_A = [
    ("Single_MLP(sklearn)", "sk"),
    ("Single_RF(sklearn)", "sk"),
    ("Single_SVR(sklearn)", "sk"),
    ("Single_Transformer(torch)", "torch"),
    ("Single_ToyMambaSSM(torch)", "torch"),
]

for target_name, target_idx in TARGETS:
    print(f"\n--- Target = {target_name} ---")
    ytr = Y_train_2[:, target_idx].astype(np.float32)
    yte = Y_test_2[:, target_idx].astype(np.float32)

    outerA = tqdm(jobs_A, desc=f"Part A models ({target_name})", leave=True)

    for job_name, job_type in outerA:
        t0 = time.time()

        if job_name.startswith("Single_MLP"):
            model = Pipeline([
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler()),
                ("reg", MLPRegressor(
                  hidden_layer_sizes=(256, 256, 128, 64),
                  activation="relu",
                  alpha=1e-4,          # L2 正則
                  learning_rate="adaptive",
                  early_stopping=True,
                  n_iter_no_change=30,
                  max_iter=5000,
                  random_state=42
                  ))
            ])
            model.fit(X_train_raw, ytr)
            pred_tr = model.predict(X_train_raw)
            pred_te = model.predict(X_test_raw)

        elif job_name.startswith("Single_RF"):
            model = Pipeline([
                ("imputer", SimpleImputer(strategy="median")),
                ("reg", RandomForestRegressor(
                    n_estimators=600,
                    random_state=42,
                    n_jobs=-1,
                    min_samples_leaf=2
                ))
            ])
            model.fit(X_train_raw, ytr)
            pred_tr = model.predict(X_train_raw)
            pred_te = model.predict(X_test_raw)

        elif job_name.startswith("Single_SVR"):
            model = Pipeline([
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler()),
                ("reg", SVR(C=10.0, epsilon=0.01, kernel="rbf"))
            ])
            model.fit(X_train_raw, ytr)
            pred_tr = model.predict(X_train_raw)
            pred_te = model.predict(X_test_raw)

        elif job_name.startswith("Single_Transformer"):
            tr = ParamTransformerReg(n_tokens=len(X_COLS), d_model=64, nhead=4, n_layers=2, out_dim=1)
            pred_tr, pred_te = train_torch_regressor_single_target(
                tr, ytr, epochs=EPOCHS_SINGLE_DEEP, lr=1e-3
            )

        elif job_name.startswith("Single_ToyMamba"):
            mb = ToyMambaReg(n_tokens=len(X_COLS), d_model=64, n_layers=2, out_dim=1)
            pred_tr, pred_te = train_torch_regressor_single_target(
                mb, ytr, epochs=EPOCHS_SINGLE_DEEP, lr=1e-3
            )

        else:
            raise RuntimeError("Unknown Part A job")

        seconds = time.time() - t0
        single_records.append(
            single_target_metrics(ytr, yte, pred_tr, pred_te, job_name, target_name, seconds=seconds)
        )

single_df = pd.DataFrame(single_records)

print("\n===== Single-valued regression (split targets) =====")
for tname, _ in TARGETS:
    print(f"\n--- {tname} ---")
    print(single_df[single_df["target"] == tname].sort_values("test_R2", ascending=False).to_string(index=False))

single_df.to_csv("compare_5models_single_split_targets.csv", index=False)
print("\nSaved: compare_5models_single_split_targets.csv")


# ============================================================
# Part B: Distribution output (MDN concept) models (split targets)
# ============================================================

def diag_gauss_logpdf(y, mu, log_sigma):
    """
    y: (B,1) or (B,)
    mu, log_sigma: (B,K,1)
    returns: (B,K)
    """
    if y.ndim == 1:
        y = y.unsqueeze(-1)
    sigma = torch.exp(log_sigma)
    y_ = y.unsqueeze(1)  # (B,1,1)
    term = -0.5 * (((y_ - mu) / sigma) ** 2) - log_sigma - 0.5 * math.log(2 * math.pi)
    return term.sum(dim=-1)  # (B,K) (sum over out_dim=1)

def mix_nll(y, logits_pi, mu, log_sigma):
    log_pi = F.log_softmax(logits_pi, dim=-1)  # (B,K)
    log_comp = diag_gauss_logpdf(y, mu, log_sigma)  # (B,K)
    log_mix = torch.logsumexp(log_pi + log_comp, dim=-1)  # (B,)
    return -log_mix.mean()

@torch.no_grad()
def mix_mean(logits_pi, mu):
    pi = F.softmax(logits_pi, dim=-1)  # (B,K)
    return (pi.unsqueeze(-1) * mu).sum(dim=1)  # (B,1)

class MDNHead(nn.Module):
    def __init__(self, in_dim, K=5, out_dim=1):
        super().__init__()
        self.K = K
        self.out_dim = out_dim
        self.fc = nn.Sequential(
            nn.Linear(in_dim, 128),
            nn.GELU(),
            nn.Linear(128, K + K*out_dim + K*out_dim)
        )

    def forward(self, z):
        K, D = self.K, self.out_dim
        p = self.fc(z)
        logits_pi = p[:, :K]
        mu = p[:, K: K + K*D].reshape(-1, K, D)
        log_sigma = p[:, K + K*D: K + 2*K*D].reshape(-1, K, D)
        log_sigma = torch.clamp(log_sigma, -8.0, 6.0)
        return logits_pi, mu, log_sigma

class MLP_MDN(nn.Module):
    def __init__(self, in_dim=10, K=5, out_dim=1):
        super().__init__()
        self.backbone = nn.Sequential(
            nn.Linear(in_dim, 128),
            nn.Tanh(),
            nn.Linear(128, 128),
            nn.Tanh(),
        )
        self.mdn = MDNHead(128, K=K, out_dim=out_dim)

    def forward(self, x):
        return self.mdn(self.backbone(x))

class Transformer_MDN(nn.Module):
    def __init__(self, n_tokens=10, d_model=64, nhead=4, n_layers=2, K=5, out_dim=1):
        super().__init__()
        self.embed = nn.Linear(1, d_model)
        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=4*d_model,
            batch_first=True, activation="gelu", dropout=0.1
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)
        self.mdn = MDNHead(in_dim=n_tokens*d_model, K=K, out_dim=out_dim)

    def forward(self, x):
        z = self.embed(x.unsqueeze(-1))
        z = self.encoder(z)
        z = z.flatten(1)
        return self.mdn(z)

class ToyMamba_MDN(nn.Module):
    def __init__(self, n_tokens=10, d_model=64, n_layers=2, K=5, out_dim=1):
        super().__init__()
        self.embed = nn.Linear(1, d_model)
        self.layers = nn.ModuleList([ToyMambaSSM(d_model=d_model) for _ in range(n_layers)])
        self.norm = nn.LayerNorm(d_model)
        self.mdn = MDNHead(in_dim=n_tokens*d_model, K=K, out_dim=out_dim)

    def forward(self, x):
        z = self.embed(x.unsqueeze(-1))
        for layer in self.layers:
            z = z + layer(z)
        z = self.norm(z)
        z = z.flatten(1)
        return self.mdn(z)

def train_neural_mdn_single_target(model, y_train_1d, y_test_1d, epochs=200, lr=1e-3, wd=1e-4):

    Ytr1 = torch.tensor(y_train_1d.reshape(-1, 1), dtype=torch.float32)
    Yte1 = torch.tensor(y_test_1d.reshape(-1, 1), dtype=torch.float32)

    train_loader = DataLoader(TensorDataset(Xtr_t, Ytr1), batch_size=BATCH_SIZE, shuffle=True)

    model = model.to(device)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)

    pbar = tqdm(range(epochs), desc=f"[Dist] {model.__class__.__name__}", leave=True)
    for _ in pbar:
        model.train()
        running = 0.0
        n = 0
        for xb, yb in train_loader:
            xb = xb.to(device)
            yb = yb.to(device)
            opt.zero_grad()
            lp, mu, ls = model(xb)
            loss = mix_nll(yb, lp, mu, ls)
            loss.backward()
            opt.step()
            running += float(loss.item())
            n += 1
        pbar.set_postfix(nll=f"{(running/max(n,1)):.4g}")

    model.eval()
    with torch.no_grad():
        lp_tr, mu_tr, ls_tr = model(Xtr_t.to(device))
        lp_te, mu_te, ls_te = model(Xte_t.to(device))

        yhat_tr = mix_mean(lp_tr, mu_tr).cpu().numpy().reshape(-1)
        yhat_te = mix_mean(lp_te, mu_te).cpu().numpy().reshape(-1)

        nll_tr = float(mix_nll(Ytr1.to(device), lp_tr, mu_tr, ls_tr).cpu().item())
        nll_te = float(mix_nll(Yte1.to(device), lp_te, mu_te, ls_te).cpu().item())

    return yhat_tr, yhat_te, nll_tr, nll_te


def rf_mixture_fit_predict_single_target(Xtr_raw, ytr, Xte_raw, yte, n_trees=80, seed=42):
    rf = RandomForestRegressor(
        n_estimators=n_trees, random_state=seed, n_jobs=-1, min_samples_leaf=2
    )
    rf.fit(Xtr_raw, ytr)

    mu_tr = np.stack([t.predict(Xtr_raw) for t in rf.estimators_], axis=1)  # (Ntr,K)
    mu_te = np.stack([t.predict(Xte_raw) for t in rf.estimators_], axis=1)  # (Nte,K)

    yhat_tr = mu_tr.mean(axis=1)
    resid = ytr - yhat_tr
    sigma = np.std(resid) + 1e-6

    K = mu_tr.shape[1]
    logits_pi_tr = np.zeros((mu_tr.shape[0], K), dtype=np.float32)
    logits_pi_te = np.zeros((mu_te.shape[0], K), dtype=np.float32)

    # reshape to (B,K,1)
    mu_tr_ = mu_tr[..., None].astype(np.float32)
    mu_te_ = mu_te[..., None].astype(np.float32)
    log_sigma_tr = (np.log(sigma) * np.ones_like(mu_tr_)).astype(np.float32)
    log_sigma_te = (np.log(sigma) * np.ones_like(mu_te_)).astype(np.float32)

    yhat_te = mu_te.mean(axis=1)

    with torch.no_grad():
        ytr_t = torch.tensor(ytr.reshape(-1, 1), dtype=torch.float32)
        yte_t = torch.tensor(yte.reshape(-1, 1), dtype=torch.float32)

        lp_tr = torch.tensor(logits_pi_tr, dtype=torch.float32)
        lp_te = torch.tensor(logits_pi_te, dtype=torch.float32)

        mu_tr_t = torch.tensor(mu_tr_, dtype=torch.float32)
        mu_te_t = torch.tensor(mu_te_, dtype=torch.float32)

        ls_tr_t = torch.tensor(log_sigma_tr, dtype=torch.float32)
        ls_te_t = torch.tensor(log_sigma_te, dtype=torch.float32)

        nll_tr = float(mix_nll(ytr_t, lp_tr, mu_tr_t, ls_tr_t).item())
        nll_te = float(mix_nll(yte_t, lp_te, mu_te_t, ls_te_t).item())

    return yhat_tr, yhat_te, nll_tr, nll_te


def svr_mixture_fit_predict_single_target(Xtr_scaled, ytr, Xte_scaled, yte, n_members=30, seed=42):
    rng = np.random.default_rng(seed)
    N = Xtr_scaled.shape[0]
    mus_tr, mus_te = [], []

    base_iter = tqdm(range(n_members), desc="[Dist] SVR bootstrap members", leave=True)
    for _ in base_iter:
        idx = rng.integers(0, N, size=N)
        model = SVR(C=10.0, epsilon=0.01, kernel="rbf")
        model.fit(Xtr_scaled[idx], ytr[idx])
        mus_tr.append(model.predict(Xtr_scaled))
        mus_te.append(model.predict(Xte_scaled))

    mu_tr = np.stack(mus_tr, axis=1)  # (Ntr,K)
    mu_te = np.stack(mus_te, axis=1)  # (Nte,K)

    yhat_tr = mu_tr.mean(axis=1)
    resid = ytr - yhat_tr
    sigma = np.std(resid) + 1e-6

    K = mu_tr.shape[1]
    logits_pi_tr = np.zeros((mu_tr.shape[0], K), dtype=np.float32)
    logits_pi_te = np.zeros((mu_te.shape[0], K), dtype=np.float32)

    mu_tr_ = mu_tr[..., None].astype(np.float32)
    mu_te_ = mu_te[..., None].astype(np.float32)
    log_sigma_tr = (np.log(sigma) * np.ones_like(mu_tr_)).astype(np.float32)
    log_sigma_te = (np.log(sigma) * np.ones_like(mu_te_)).astype(np.float32)

    yhat_te = mu_te.mean(axis=1)

    with torch.no_grad():
        ytr_t = torch.tensor(ytr.reshape(-1, 1), dtype=torch.float32)
        yte_t = torch.tensor(yte.reshape(-1, 1), dtype=torch.float32)

        lp_tr = torch.tensor(logits_pi_tr, dtype=torch.float32)
        lp_te = torch.tensor(logits_pi_te, dtype=torch.float32)

        mu_tr_t = torch.tensor(mu_tr_, dtype=torch.float32)
        mu_te_t = torch.tensor(mu_te_, dtype=torch.float32)

        ls_tr_t = torch.tensor(log_sigma_tr, dtype=torch.float32)
        ls_te_t = torch.tensor(log_sigma_te, dtype=torch.float32)

        nll_tr = float(mix_nll(ytr_t, lp_tr, mu_tr_t, ls_tr_t).item())
        nll_te = float(mix_nll(yte_t, lp_te, mu_te_t, ls_te_t).item())

    return yhat_tr, yhat_te, nll_tr, nll_te


print("\nRunning Part B: distribution outputs (MDN concept) (split targets)...")

dist_records = []
jobs_B = [
    ("Dist_MLP_MDN(torch)", "torch"),
    ("Dist_RF_MixtureTrees", "mix"),
    ("Dist_SVR_MixtureBootstrap", "mix"),
    ("Dist_Transformer_MDN(torch)", "torch"),
    ("Dist_ToyMambaSSM_MDN(torch)", "torch"),
]

for target_name, target_idx in TARGETS:
    print(f"\n--- Target = {target_name} ---")
    ytr = Y_train_2[:, target_idx].astype(np.float32)
    yte = Y_test_2[:, target_idx].astype(np.float32)

    outerB = tqdm(jobs_B, desc=f"Part B models ({target_name})", leave=True)

    for job_name, job_type in outerB:
        t0 = time.time()

        if job_name.startswith("Dist_MLP_MDN"):
            model = MLP_MDN(in_dim=len(X_COLS), K=5, out_dim=1)
            pred_tr, pred_te, nll_tr, nll_te = train_neural_mdn_single_target(
                model, ytr, yte, epochs=EPOCHS_DIST_DEEP, lr=1e-3
            )

        elif job_name.startswith("Dist_RF"):
            pred_tr, pred_te, nll_tr, nll_te = rf_mixture_fit_predict_single_target(
                X_train_raw, ytr, X_test_raw, yte, n_trees=80, seed=42
            )

        elif job_name.startswith("Dist_SVR"):
            pred_tr, pred_te, nll_tr, nll_te = svr_mixture_fit_predict_single_target(
                X_train_scaled, ytr, X_test_scaled, yte, n_members=30, seed=42
            )

        elif job_name.startswith("Dist_Transformer"):
            model = Transformer_MDN(n_tokens=len(X_COLS), d_model=64, nhead=4, n_layers=2, K=5, out_dim=1)
            pred_tr, pred_te, nll_tr, nll_te = train_neural_mdn_single_target(
                model, ytr, yte, epochs=EPOCHS_DIST_DEEP, lr=1e-3
            )

        elif job_name.startswith("Dist_ToyMamba"):
            model = ToyMamba_MDN(n_tokens=len(X_COLS), d_model=64, n_layers=2, K=5, out_dim=1)
            pred_tr, pred_te, nll_tr, nll_te = train_neural_mdn_single_target(
                model, ytr, yte, epochs=EPOCHS_DIST_DEEP, lr=1e-3
            )

        else:
            raise RuntimeError("Unknown Part B job")

        seconds = time.time() - t0

        extra = {"train_NLL": nll_tr, "test_NLL": nll_te}
        dist_records.append(
            single_target_metrics(ytr, yte, pred_tr, pred_te, job_name, target_name, seconds=seconds, extra=extra)
        )

dist_df = pd.DataFrame(dist_records)

print("\n===== Distribution output (MDN concept) (split targets) =====")
for tname, _ in TARGETS:
    print(f"\n--- {tname} ---")
    print(dist_df[dist_df["target"] == tname].sort_values("test_R2", ascending=False).to_string(index=False))

dist_df.to_csv("compare_5models_distribution_split_targets.csv", index=False)
print("\nSaved: compare_5models_distribution_split_targets.csv")

import os
import math
import time
import random

import numpy as np
import pandas as pd
from tqdm.auto import tqdm

from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import TransformedTargetRegressor
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error

from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor
from sklearn.svm import SVR

import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader


# ---------------------------
# Reproducibility
# ---------------------------
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(42)
device = "cuda" if torch.cuda.is_available() else "cpu"
print("device =", device)


# ---------------------------
# Config
# ---------------------------
CSV_PATH = "nonunique_6000_io.csv"

X_COLS = ["sigma0", "ramp", "omega", "cyc_frac", "Y0", "A_rate", "D_c", "m_sub", "a0", "h"]
TARGETS = [("t_fp", 0), ("a_fp", 1)]

EPOCHS_SINGLE_DEEP = 200
BATCH_SIZE = 256


# ---------------------------
# Load data
# ---------------------------
if not os.path.exists(CSV_PATH):
    raise FileNotFoundError(f"Cannot find {CSV_PATH}. Please put it next to this script.")

df = pd.read_csv(CSV_PATH)
missing = [c for c in X_COLS + [t for t, _ in TARGETS] if c not in df.columns]
if missing:
    raise ValueError(f"CSV missing columns: {missing}")

df = df[X_COLS + [t for t, _ in TARGETS]].copy()
df = df.dropna(subset=[t for t, _ in TARGETS])

X = df[X_COLS].to_numpy(dtype=np.float32)
Y = df[[t for t, _ in TARGETS]].to_numpy(dtype=np.float32)  # (N,2)

X_train_raw, X_test_raw, Y_train_2, Y_test_2 = train_test_split(
    X, Y, test_size=0.2, random_state=42
)

print("Train:", X_train_raw.shape, Y_train_2.shape)
print("Test :", X_test_raw.shape,  Y_test_2.shape)


# ---------------------------
# Metrics helper (single output)
# ---------------------------
def single_target_metrics(ytr, yte, yhat_tr, yhat_te, model_name, target_name, seconds=None, extra=None):
    rec = {
        "model": model_name,
        "target": target_name,
        "train_R2": float(r2_score(ytr, yhat_tr)),
        "test_R2":  float(r2_score(yte, yhat_te)),
        "train_MSE": float(mean_squared_error(ytr, yhat_tr)),
        "test_MSE":  float(mean_squared_error(yte, yhat_te)),
    }
    if seconds is not None:
        rec["seconds"] = float(seconds)
    if extra:
        for k, v in extra.items():
            rec[k] = float(v)
    return rec


imp = SimpleImputer(strategy="median")
sc = StandardScaler()

X_train_scaled = sc.fit_transform(imp.fit_transform(X_train_raw))
X_test_scaled  = sc.transform(imp.transform(X_test_raw))

Xtr_t = torch.tensor(X_train_scaled, dtype=torch.float32)
Xte_t = torch.tensor(X_test_scaled, dtype=torch.float32)


# ============================================================
# Torch single-valued models (out_dim=1)
# ============================================================
class ParamTransformerReg(nn.Module):
    def __init__(self, n_tokens=10, d_model=64, nhead=4, n_layers=2, out_dim=1):
        super().__init__()
        self.embed = nn.Linear(1, d_model)
        enc_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=4*d_model,
            batch_first=True, activation="gelu", dropout=0.1
        )
        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)
        self.head = nn.Sequential(
            nn.Linear(n_tokens * d_model, 128),
            nn.GELU(),
            nn.Linear(128, out_dim)
        )

    def forward(self, x):
        z = self.embed(x.unsqueeze(-1))
        z = self.encoder(z)
        z = z.flatten(1)
        return self.head(z)


class ToyMambaSSM(nn.Module):
    def __init__(self, d_model=64):
        super().__init__()
        self.Wx = nn.Linear(d_model, d_model)
        self.Wh = nn.Linear(d_model, d_model, bias=False)
        self.Wg = nn.Linear(d_model, d_model)

    def forward(self, x):
        B, T, D = x.shape
        h = torch.zeros(B, D, device=x.device, dtype=x.dtype)
        outs = []
        for t in range(T):
            xt = x[:, t, :]
            g = torch.sigmoid(self.Wg(xt))
            h_tilde = torch.tanh(self.Wx(xt) + self.Wh(h))
            h = (1.0 - g) * h + g * h_tilde
            outs.append(h.unsqueeze(1))
        return torch.cat(outs, dim=1)


class ToyMambaReg(nn.Module):
    def __init__(self, n_tokens=10, d_model=64, n_layers=2, out_dim=1):
        super().__init__()
        self.embed = nn.Linear(1, d_model)
        self.layers = nn.ModuleList([ToyMambaSSM(d_model=d_model) for _ in range(n_layers)])
        self.norm = nn.LayerNorm(d_model)
        self.head = nn.Sequential(
            nn.Linear(n_tokens * d_model, 128),
            nn.GELU(),
            nn.Linear(128, out_dim)
        )

    def forward(self, x):
        z = self.embed(x.unsqueeze(-1))
        for layer in self.layers:
            z = z + layer(z)
        z = self.norm(z)
        z = z.flatten(1)
        return self.head(z)


def train_torch_regressor_single_target(model, y_train_1d, epochs=200, lr=1e-3, wd=1e-4):
    Ytr1 = torch.tensor(y_train_1d.reshape(-1, 1), dtype=torch.float32)
    train_loader = DataLoader(TensorDataset(Xtr_t, Ytr1), batch_size=BATCH_SIZE, shuffle=True)

    model = model.to(device)
    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)
    loss_fn = nn.MSELoss()

    pbar = tqdm(range(epochs), desc=f"[Single] {model.__class__.__name__}", leave=True)
    for _ in pbar:
        model.train()
        running = 0.0
        n = 0
        for xb, yb in train_loader:
            xb = xb.to(device)
            yb = yb.to(device)
            opt.zero_grad()
            pred = model(xb)
            loss = loss_fn(pred, yb)
            loss.backward()
            opt.step()
            running += float(loss.item())
            n += 1
        pbar.set_postfix(loss=f"{(running/max(n,1)):.4g}")

    model.eval()
    with torch.no_grad():
        pred_tr = model(Xtr_t.to(device)).cpu().numpy().reshape(-1)
        pred_te = model(Xte_t.to(device)).cpu().numpy().reshape(-1)
    return pred_tr, pred_te


# ============================================================
# Part A: Single-valued regression (split targets) + improvements
# ============================================================
print("\nRunning Part A: single-valued regression (split targets, improved baselines)...")

records = []

for target_name, target_idx in TARGETS:
    print(f"\n--- Target = {target_name} ---")
    ytr = Y_train_2[:, target_idx].astype(np.float32)
    yte = Y_test_2[:, target_idx].astype(np.float32)

    jobs = []

    # Baselines (same spirit as original)
    jobs.append(("Single_MLP_shallow(sklearn)", "sk_mlp_shallow"))
    jobs.append(("Single_MLP_deep_tabular(sklearn)", "sk_mlp_deep"))
    jobs.append(("Single_RF(sklearn)", "sk_rf"))
    jobs.append(("Single_SVR(sklearn)", "sk_svr"))
    jobs.append(("Single_HistGBDT(sklearn)", "sk_hgb"))

    # Optional target-transform variant (very low-cost, often helps long-tail targets)
    if target_name == "t_fp":
        jobs.append(("Single_HistGBDT_log1pT(sklearn)", "sk_hgb_logt"))

    # Torch models (keep for comparison)
    jobs.append(("Single_Transformer(torch)", "torch_tr"))
    jobs.append(("Single_ToyMambaSSM(torch)", "torch_mb"))

    outer = tqdm(jobs, desc=f"Models ({target_name})", leave=True)

    for job_name, job_type in outer:
        t0 = time.time()

        if job_type == "sk_mlp_shallow":
            model = Pipeline([
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler()),
                ("reg", MLPRegressor(
                    hidden_layer_sizes=(128, 128),
                    activation="tanh",
                    solver="adam",
                    max_iter=5000,
                    random_state=42
                ))
            ])
            model.fit(X_train_raw, ytr)
            pred_tr = model.predict(X_train_raw)
            pred_te = model.predict(X_test_raw)

        elif job_type == "sk_mlp_deep":
            # Deeper, tabular-friendly settings: relu + early stopping + L2
            model = Pipeline([
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler()),
                ("reg", MLPRegressor(
                    hidden_layer_sizes=(256, 256, 128, 64),
                    activation="relu",
                    solver="adam",
                    alpha=1e-4,
                    learning_rate="adaptive",
                    early_stopping=True,
                    n_iter_no_change=30,
                    max_iter=5000,
                    random_state=42
                ))
            ])
            model.fit(X_train_raw, ytr)
            pred_tr = model.predict(X_train_raw)
            pred_te = model.predict(X_test_raw)

        elif job_type == "sk_rf":
            model = Pipeline([
                ("imputer", SimpleImputer(strategy="median")),
                ("reg", RandomForestRegressor(
                    n_estimators=800,
                    random_state=42,
                    n_jobs=-1,
                    min_samples_leaf=2
                ))
            ])
            model.fit(X_train_raw, ytr)
            pred_tr = model.predict(X_train_raw)
            pred_te = model.predict(X_test_raw)

        elif job_type == "sk_svr":
            model = Pipeline([
                ("imputer", SimpleImputer(strategy="median")),
                ("scaler", StandardScaler()),
                ("reg", SVR(C=10.0, epsilon=0.01, kernel="rbf"))
            ])
            model.fit(X_train_raw, ytr)
            pred_tr = model.predict(X_train_raw)
            pred_te = model.predict(X_test_raw)

        elif job_type == "sk_hgb":
            # Strong baseline for numeric tabular
            model = Pipeline([
                ("imputer", SimpleImputer(strategy="median")),
                ("reg", HistGradientBoostingRegressor(
                    max_depth=6,
                    learning_rate=0.05,
                    max_iter=800,
                    l2_regularization=1e-3,
                    validation_fraction=0.1,
                    n_iter_no_change=30,
                    random_state=42
                ))
            ])
            model.fit(X_train_raw, ytr)
            pred_tr = model.predict(X_train_raw)
            pred_te = model.predict(X_test_raw)

        elif job_type == "sk_hgb_logt":
            # Transform target only, keep X pipeline minimal
            base = Pipeline([
                ("imputer", SimpleImputer(strategy="median")),
                ("reg", HistGradientBoostingRegressor(
                    max_depth=6,
                    learning_rate=0.05,
                    max_iter=800,
                    l2_regularization=1e-3,
                    validation_fraction=0.1,
                    n_iter_no_change=30,
                    random_state=42
                ))
            ])
            model = TransformedTargetRegressor(
                regressor=base,
                func=np.log1p,
                inverse_func=np.expm1
            )
            model.fit(X_train_raw, ytr)
            pred_tr = model.predict(X_train_raw)
            pred_te = model.predict(X_test_raw)

        elif job_type == "torch_tr":
            tr = ParamTransformerReg(n_tokens=len(X_COLS), d_model=64, nhead=4, n_layers=2, out_dim=1)
            pred_tr, pred_te = train_torch_regressor_single_target(tr, ytr, epochs=EPOCHS_SINGLE_DEEP, lr=1e-3)

        elif job_type == "torch_mb":
            mb = ToyMambaReg(n_tokens=len(X_COLS), d_model=64, n_layers=2, out_dim=1)
            pred_tr, pred_te = train_torch_regressor_single_target(mb, ytr, epochs=EPOCHS_SINGLE_DEEP, lr=1e-3)

        else:
            raise RuntimeError(f"Unknown job_type: {job_type}")

        seconds = time.time() - t0
        records.append(single_target_metrics(
            ytr=ytr, yte=yte,
            yhat_tr=pred_tr, yhat_te=pred_te,
            model_name=job_name,
            target_name=target_name,
            seconds=seconds
        ))

# Results
res = pd.DataFrame(records)

print("\n===== Single-valued regression (split targets, improved) =====")
for tname, _ in TARGETS:
    print(f"\n--- {tname} ---")
    print(res[res["target"] == tname].sort_values("test_R2", ascending=False).to_string(index=False))

out_csv = "compare_single_split_targets_improved.csv"
res.to_csv(out_csv, index=False)
print(f"\nSaved: {out_csv}")

import os
import json
import random
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.compose import TransformedTargetRegressor

from sklearn.ensemble import HistGradientBoostingRegressor, GradientBoostingRegressor
from sklearn.metrics import (
    r2_score,
    mean_squared_error,
    mean_absolute_error,
    mean_pinball_loss,
)

SEED = 42
random.seed(SEED)
np.random.seed(SEED)

CSV_PATH = "nonunique_6000_io.csv"

X_COLS = ["sigma0", "ramp", "omega", "cyc_frac", "Y0", "A_rate", "D_c", "m_sub", "a0", "h"]
T_FP_COL = "t_fp"
A_FP_COL = "a_fp"

TEST_SIZE = 0.20

CV_SPLITS = 5
N_ITER = 60
N_JOBS = -1

QUANTILES = [0.1, 0.5, 0.9]


def assert_columns(df, cols):
    missing = [c for c in cols if c not in df.columns]
    if missing:
        raise ValueError(f"CSV missing columns: {missing}")

def save_json(path, obj):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)

def summarize_regression(y_true, y_pred, name=""):
    return {
        "name": name,
        "R2": float(r2_score(y_true, y_pred)),
        "MSE": float(mean_squared_error(y_true, y_pred)),
        "MAE": float(mean_absolute_error(y_true, y_pred)),
    }

if not os.path.exists(CSV_PATH):
    raise FileNotFoundError(f"Cannot find {CSV_PATH}. Put it next to this script.")

df = pd.read_csv(CSV_PATH)
assert_columns(df, X_COLS + [T_FP_COL, A_FP_COL])

df = df[X_COLS + [T_FP_COL, A_FP_COL]].copy()
df = df.dropna(subset=[T_FP_COL, A_FP_COL])

X = df[X_COLS].to_numpy(dtype=np.float32)
y_t = df[T_FP_COL].to_numpy(dtype=np.float32)
y_a = df[A_FP_COL].to_numpy(dtype=np.float32)

X_train, X_test, y_t_train, y_t_test, y_a_train, y_a_test = train_test_split(
    X, y_t, y_a, test_size=TEST_SIZE, random_state=SEED
)

print("Train X:", X_train.shape, "Test X:", X_test.shape)
print("Train a_fp:", y_a_train.shape, "Test a_fp:", y_a_test.shape)
print("Train t_fp:", y_t_train.shape, "Test t_fp:", y_t_test.shape)

print("\n=== Part 1: a_fp hyperparameter search ===")

cv = KFold(n_splits=CV_SPLITS, shuffle=True, random_state=SEED)

pipe_a_raw = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("reg", HistGradientBoostingRegressor(
        random_state=SEED
    ))
])

# HGB 的參數空間。這些是 tabular regression 常用、而且對泛化影響很大的一組。
param_dist_raw = {
    "reg__max_depth": [3, 4, 5, 6, 8, 10, None],
    "reg__learning_rate": [0.01, 0.02, 0.03, 0.05, 0.08, 0.1],
    "reg__max_iter": [300, 500, 800, 1200, 1600],
    "reg__min_samples_leaf": [5, 10, 20, 30, 50],
    "reg__l2_regularization": [0.0, 1e-5, 1e-4, 1e-3, 1e-2],
    "reg__max_bins": [64, 128, 255],
    "reg__max_leaf_nodes": [15, 31, 63, 127, None],
}

search_raw = RandomizedSearchCV(
    estimator=pipe_a_raw,
    param_distributions=param_dist_raw,
    n_iter=N_ITER,
    scoring="r2",
    cv=cv,
    random_state=SEED,
    n_jobs=N_JOBS,
    verbose=1
)

def safe_sqrt(x):
    return np.sqrt(np.clip(x, 0.0, None))

def inv_safe_sqrt(x):
    return np.square(np.clip(x, 0.0, None))

pipe_a_sqrt = TransformedTargetRegressor(
    regressor=pipe_a_raw,
    func=safe_sqrt,
    inverse_func=inv_safe_sqrt,
    check_inverse=False
)


param_dist_sqrt = {f"regressor__{k}": v for k, v in param_dist_raw.items()}

search_sqrt = RandomizedSearchCV(
    estimator=pipe_a_sqrt,
    param_distributions=param_dist_sqrt,
    n_iter=N_ITER,
    scoring="r2",
    cv=cv,
    random_state=SEED,
    n_jobs=N_JOBS,
    verbose=1
)

print("\n[Search] a_fp with raw target")
search_raw.fit(X_train, y_a_train)

print("\n[Search] a_fp with sqrt-transformed target")
search_sqrt.fit(X_train, y_a_train)

best_raw_cv = search_raw.best_score_
best_sqrt_cv = search_sqrt.best_score_

if best_sqrt_cv >= best_raw_cv:
    best_search = search_sqrt
    best_kind = "HGBR_sqrt_target"
else:
    best_search = search_raw
    best_kind = "HGBR_raw_target"

best_model_a = best_search.best_estimator_
print("\nBest a_fp model kind:", best_kind)
print("Best CV R2:", float(best_search.best_score_))
print("Best params:", best_search.best_params_)

best_model_a.fit(X_train, y_a_train)
pred_a_train = best_model_a.predict(X_train)
pred_a_test = best_model_a.predict(X_test)

a_train_metrics = summarize_regression(y_a_train, pred_a_train, name="a_fp train")
a_test_metrics = summarize_regression(y_a_test, pred_a_test, name="a_fp test")

print("\n[a_fp] Train metrics:", a_train_metrics)
print("[a_fp] Test metrics :", a_test_metrics)


out_a = pd.DataFrame({
    "a_fp_true": y_a_test,
    "a_fp_pred": pred_a_test
})
out_a.to_csv("a_fp_predictions_test.csv", index=False)

save_json("a_fp_best_model_summary.json", {
    "best_kind": best_kind,
    "best_cv_r2": float(best_search.best_score_),
    "best_params": best_search.best_params_,
    "train_metrics": a_train_metrics,
    "test_metrics": a_test_metrics,
})

print("\nSaved: a_fp_predictions_test.csv")
print("Saved: a_fp_best_model_summary.json")

quantile_models = {}
quantile_preds_test = {}

for q in QUANTILES:
    model_q = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("reg", GradientBoostingRegressor(
            loss="quantile",
            alpha=q,
            random_state=SEED,
            n_estimators=800,
            learning_rate=0.03,
            max_depth=3,
            min_samples_leaf=20,
            subsample=0.9
        ))
    ])
    model_q.fit(X_train, y_t_train)
    quantile_models[q] = model_q
    quantile_preds_test[q] = model_q.predict(X_test)

out_t = pd.DataFrame({
    "t_fp_true": y_t_test,
})
for q in QUANTILES:
    col = f"t_fp_q{int(q*100):02d}"
    out_t[col] = quantile_preds_test[q]

out_t.to_csv("t_fp_quantile_predictions_test.csv", index=False)
print("\nSaved: t_fp_quantile_predictions_test.csv")

pinball = {}
for q in QUANTILES:
    pinball[q] = float(mean_pinball_loss(y_t_test, quantile_preds_test[q], alpha=q))

q_low, q_high = 0.1, 0.9
low = quantile_preds_test[q_low]
high = quantile_preds_test[q_high]
coverage = float(np.mean((y_t_test >= low) & (y_t_test <= high)))
interval_width = float(np.mean(high - low))

med = quantile_preds_test[0.5]
median_mae = float(mean_absolute_error(y_t_test, med))

t_fp_summary = {
    "pinball_loss": {str(k): v for k, v in pinball.items()},
    "coverage_q10_q90": coverage,
    "avg_interval_width_q10_q90": interval_width,
    "median_mae_q50": median_mae,
    "median_r2_q50": float(r2_score(y_t_test, med)),
}

print("\n[t_fp] Quantile summary:")
print(t_fp_summary)

save_json("t_fp_quantile_summary.json", t_fp_summary)
print("\nSaved: t_fp_quantile_summary.json")

final_summary = {
    "a_fp_best_kind": best_kind,
    "a_fp_best_cv_r2": float(best_search.best_score_),
    "a_fp_test_R2": a_test_metrics["R2"],
    "t_fp_quantile_summary": t_fp_summary
}
save_json("final_summary.json", final_summary)
print("\nSaved: final_summary.json")
print("\nDone.")

import numpy as np
import pandas as pd
from sklearn.neighbors import NearestNeighbors

CSV_PATH = "nonunique_6000_io.csv"

X_COLS = ["sigma0", "ramp", "omega", "cyc_frac",
          "Y0", "A_rate", "D_c", "m_sub", "a0", "h"]

Y_TFP = "t_fp"
Y_AFP = "a_fp"

df = pd.read_csv(CSV_PATH)

missing = [c for c in X_COLS + [Y_TFP, Y_AFP] if c not in df.columns]
if missing:
    raise ValueError(f"CSV missing columns: {missing}")

df = df[X_COLS + [Y_TFP, Y_AFP]].dropna()

X = df[X_COLS].to_numpy(dtype=np.float32)
y_t_fp = df[Y_TFP].to_numpy(dtype=np.float32)
y_a_fp = df[Y_AFP].to_numpy(dtype=np.float32)

print("Loaded data:")
print("X shape     =", X.shape)
print("y_t_fp shape =", y_t_fp.shape)
print("y_a_fp shape =", y_a_fp.shape)

def conditional_entropy_Y_given_X(X, y, k=30, n_bins=10):
    """
    Estimate average conditional entropy H(Y | X≈x)
    using k-NN neighborhoods.
    """
    nbrs = NearestNeighbors(n_neighbors=k).fit(X)
    _, indices = nbrs.kneighbors(X)

    entropies = []

    for neigh in indices:
        y_neigh = y[neigh]
        hist, _ = np.histogram(y_neigh, bins=n_bins, density=True)
        p = hist + 1e-12
        p = p / p.sum()
        H = -np.sum(p * np.log(p))
        entropies.append(H)

    return float(np.mean(entropies)), float(np.std(entropies))


H_tfp, std_tfp = conditional_entropy_Y_given_X(X, y_t_fp)
H_afp, std_afp = conditional_entropy_Y_given_X(X, y_a_fp)

print("\nConditional entropy results")
print(f"H(t_fp | X) = {H_tfp:.3f} ± {std_tfp:.3f}")
print(f"H(a_fp | X) = {H_afp:.3f} ± {std_afp:.3f}")
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(
    n_estimators=200,
    random_state=0,
    min_samples_leaf=5
)
rf.fit(X, y_t_fp)
y_t_hat = rf.predict(X)

H_t_hat, _ = conditional_entropy_Y_given_X(X, y_t_hat)
print("H(det surrogate t_fp | X) =", H_t_hat)

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# -----------------------------
# User settings
# -----------------------------
CSV_PATH = "t_fp_quantile_predictions_test.csv"   # change if needed
OUT_PNG  = "fig_tfp_quantile_band_q10_q90.png"    # output figure
SORT_BY  = None  # set to "q50" to sort by predicted median, or None to keep original order

# -----------------------------
# Helpers
# -----------------------------
def pick_col(df: pd.DataFrame, candidates):
    """Pick the first existing column from candidates (case-insensitive)."""
    cols_lower = {c.lower(): c for c in df.columns}
    for cand in candidates:
        if cand.lower() in cols_lower:
            return cols_lower[cand.lower()]
    return None

# -----------------------------
# Load
# -----------------------------
if not os.path.exists(CSV_PATH):
    raise FileNotFoundError(f"Cannot find '{CSV_PATH}'. Put it next to this script or update CSV_PATH.")

df = pd.read_csv(CSV_PATH)

# Try common column name patterns
y_col = pick_col(df, ["y_true", "t_fp_true", "t_true", "true", "y"])
q10_col = pick_col(df, ["q10", "p10", "quantile_0.1", "q_0.1", "pred_q10", "y_q10", "t_fp_q10"])
q50_col = pick_col(df, ["q50", "p50", "quantile_0.5", "q_0.5", "pred_q50", "y_q50", "t_fp_q50", "median"])
q90_col = pick_col(df, ["q90", "p90", "quantile_0.9", "q_0.9", "pred_q90", "y_q90", "t_fp_q90"])

missing = [name for name, col in [("y_true", y_col), ("q10", q10_col), ("q50", q50_col), ("q90", q90_col)] if col is None]
if missing:
    raise ValueError(
        "Missing required columns in CSV. Need y_true, q10, q50, q90 (or equivalent names).\n"
        f"Columns found: {list(df.columns)}\n"
        f"Missing: {missing}"
    )

# Clean numeric
for c in [y_col, q10_col, q50_col, q90_col]:
    df[c] = pd.to_numeric(df[c], errors="coerce")

df = df.dropna(subset=[y_col, q10_col, q50_col, q90_col]).reset_index(drop=True)

# Optional sorting (sometimes makes the band easier to read)
if SORT_BY == "q50":
    df = df.sort_values(q50_col).reset_index(drop=True)

# -----------------------------
# Metrics (optional console info)
# -----------------------------
y = df[y_col].to_numpy()
q10 = df[q10_col].to_numpy()
q50 = df[q50_col].to_numpy()
q90 = df[q90_col].to_numpy()

coverage = np.mean((y >= q10) & (y <= q90))
avg_width = float(np.mean(q90 - q10))

print(f"Samples plotted: {len(df)}")
print(f"Coverage (Q10–Q90): {coverage:.4f}")
print(f"Avg interval width: {avg_width:.4f}")

# -----------------------------
# Plot: Quantile band figure
# -----------------------------
x = np.arange(len(df))

plt.figure(figsize=(10, 4.8))
plt.fill_between(x, q10, q90, alpha=0.25, label="Predicted Q10–Q90 band")
plt.plot(x, q50, linewidth=1.5, label="Predicted median (Q50)")
plt.scatter(x, y, s=10, alpha=0.8, label="Observed $t_{\\mathrm{fp}}$")

plt.xlabel("Test sample index" + (" (sorted by Q50)" if SORT_BY == "q50" else ""))
plt.ylabel(r"$t_{\mathrm{fp}}$")
plt.legend()
plt.tight_layout()
plt.savefig(OUT_PNG, dpi=600)
plt.show()

print(f"Saved figure: {OUT_PNG}")
